tiramisu:
    #tiramisu_path: "/home/dl5133/tiramisu/"
    tiramisu_path: "/home/ak11326/tiramisu"
    env_type:  "cpu" # model | cpu
    #tags_model_weights: "./env_api/scheduler/models/model_release_version.pt"
    tags_model_weights: "./env_api/scheduler/models/best_model_new_model.pt"
    workspace: "./experiment_dir/workspace/"

env_vars:
    #CONDA_ENV : "/scratch/ss18596/conda_envs"
    #CONDA_ENV: "/scratch/dl5133/conda-envs/tiramisu-build-env"
    #CONDA_ENV: "/scratch/ak11326/Tools/.conda/envs/djamel-env"
    CONDA_ENV : "/home/ss18596/.conda/envs/djamel-env"
    LD_LIBRARY_PATH: "${CONDA_ENV}/lib:${TIRAMISU_ROOT}/3rdParty/Halide/install/lib64:${TIRAMISU_ROOT}/3rdParty/llvm/build/lib:${TIRAMISU_ROOT}/3rdParty/isl/build/lib:"


dataset:        
    dataset_format: PICKLE
    cpps_path: "datasets/cpp_files_220k.pkl"
    dataset_path: "datasets/schedules_220k.pkl"   
    save_path: './datasets'
    shuffle: True
    seed: 13
    saving_frequency: 100
    # When doing evaluation on the benchmark set the value to True
    is_benchmark: False
    benchmark_cpp_files: "/scratch/ss18596/dev9/0_30_362_grpc/benchmark_evaluation/benchmark/cpps.pkl"
    benchmark_dataset_path: "/scratch/ss18596/dev9/0_30_362_grpc/benchmark_evaluation/benchmark/xyz.pkl"


ray:
    results: "./experiment_dir/ray_results"
    restore_checkpoint: "/scratch/ss18596/dev9/0_30_362_grpc/experiment_dir/ray_results/actions30_cpu/PPO_TiramisuRlEnv_8bee3_00000_0_2024-06-13_17-19-22/checkpoint_000100/"
    #restore_checkpoint: "/scratch/ss18596/dev9/0_30_grpc_362/experiment_dir/outputs/best_model/checkpoint_000640"

experiment:
    name: actions30_cpu
    checkpoint_frequency: 20
    checkpoint_num_to_keep: 30
    # The following 3 values are the values to stop the experiment if any of them is reached 
    training_iteration: 80000
    timesteps_total: 10000000
    episode_reward_mean: 10
    # Use this value to punish or tolerate illegal actions from being taken
    legality_speedup: 1.0
    # Use the order of beam search 
    beam_search_order : True
    entropy_coeff: 0.00001
    # Training parameters
    train_batch_size: 4096 
    minibatch_size: 128
    lr: 0.00001
    vf_loss_coeff: 1
    # Policy model type
    policy_model: "lstm" #"lstm" #| "ff" #feed-forward
    # In mode "ff" set vf_share_layers to True if you want to use shared weights between policy and value function 
    # in mode "lstm" , you must make vf_share_layers = True
    vf_share_layers: True

lstm_policy:
    fc_size: 1024
    lstm_state_size: 512
    num_layers: 1