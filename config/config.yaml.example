tiramisu:
    tiramisu_path: "path to tiramisu root directory" 
    env_type:  "model"
    tags_model_weights: "<Your path to the repo>/env_api/scheduler/models/model_release_version.pt"

dataset:
    path: 'path to dataset'
    offline : 'path to saved dataset <pkl file>, if available'
    save_path: 'path to save new pkl dataset'
    # When doing evaluation on the benchmark set the value to True
    is_benchmark: False
    benchmark_cpp_files: 'path to cpp folder of benchmarks'
    benchmark_path: 'path to pkl file of benchmarks of available'

ray:
    results: "path to where you want to store ray results"
    restore_checkpoint: "path of the checkpoint you want to restore"

experiment:
    name: "test"
    checkpoint_frequency: 10
    checkpoint_num_to_keep: 10
    # The following 3 values are the values to stop the experiment if any of them is reached 
    training_iteration: 500
    timesteps_total: 1000000
    episode_reward_mean: 2
    # Use this value to punish or tolerate illegal actions from being taken
    legality_speedup: 0.9
    # Use the order of beam search 
    beam_search_order : True

policy_network:
    # Set this to True if you want to use shared weights between policy and value function
    vf_share_layers: False
    policy_hidden_layers: 
        - 2048
        - 512 
        - 64
    # If vf_share_layers is true then, these values won't be taken for the value network
    vf_hidden_layers: 
        - 512 
        - 64
    dropout_rate: 0.2
    lr: 0.001